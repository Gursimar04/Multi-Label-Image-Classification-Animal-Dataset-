{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries required to make model\n",
    "import tensorflow as tf   \n",
    "from tensorflow import keras\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "physical_device=tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"No. of GPUs Available: \",len(physical_device))\n",
    "tf.config.experimental.set_memory_growth(physical_device[0],True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import PIL\n",
    "#CNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense,Dropout,MaxPooling2D,Conv2D,Flatten,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "#save model\n",
    "from tensorflow.keras.models import load_model\n",
    "#image preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "#dividing data into train and validation\n",
    "import os.path\n",
    "import random as rnd\n",
    "import glob\n",
    "import shutil\n",
    "#face detection test\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt               # for visualization\n",
    "%matplotlib inline\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Building a Multi-Layer Perceptron\n",
    "We have used Sequential API which helps us define the model in a step by step manner by adding multiple dense layer.\n",
    "The term kernel_initializer is a f statistical distribution that initializes weights for the model. We have used uniform distribution in this case. Softmax is being used for the final activation layer as it “returns a probability distribution over the target classes in a multiclass classification problem” \n",
    "###### Dropout is being used for optimization. It is a regularization technique in which we ommit units randomly to reduce the chances of overfitting of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "    classifier = Sequential()\n",
    "    #First Layer\n",
    "    classifier.add(Dense(units = 124, kernel_initializer = 'uniform', activation = 'relu', input_shape = (12288,), \n",
    "                         kernel_regularizer=regularizers.l2(0.01))) # L2 regularization\n",
    "    classifier.add(Dropout(0.5))\n",
    "    # Intermediate Layers\n",
    "    for itr in range(2):\n",
    "        classifier.add(Dense(units = 62, kernel_initializer = 'uniform', activation = 'relu', \n",
    "                             kernel_regularizer=regularizers.l2(0.01))) # L2 regularization\n",
    "        classifier.add(Dropout(0.5))   \n",
    "    # Last Layer\n",
    "    classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 124)               1523836   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 124)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 62)                7750      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 62)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 62)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 5)                 315       \n",
      "=================================================================\n",
      "Total params: 1,535,807\n",
      "Trainable params: 1,535,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Loss Function and Optimizer \n",
    "We have used adam for optimizaion. It is bit different from the textbook, gradient descent. Gradient descent maintains a single learning rate for all weight updates but Adam allows us to add decay which slows down the learning rate. As we move closer to our optimized value the incresed decay helps as it causes less deviation in the weights.\n",
    "\n",
    "Loss function being used here is called binary crossentropy. It is generally used for classification as it uses probability distribution to give us a 1 or 0 value. \n",
    "###### I did not use categorical crossentropy as I am manually encoding the data int one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = Adam(lr=1e-3, decay=1e-3 / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    " classifier.compile(optimizer = op , loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Loading Data \n",
    "I have created an array of all our resultant labels and then iterated through it using a simple for loop. For each iteration I have acquired the folder containing the images of that dataset and converted the images into 64,64,3 and finally flattened them to receive a shape of (12288,). Flattening has been used as we are using ANN to compute results.\n",
    "\n",
    "For label encoding I define a list with 5 zero values. I acquire the index of the current category, convert that index into 1 and finally append it to out label array.\n",
    "###### At the end i have used shuffle from sklearn to shuffle the dataset for better training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['elefante', 'farfalla','mucca','pecora','scoiattolo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "label=[]\n",
    "for class_name in class_names: # iterate through classes\n",
    "    try:\n",
    "        files = glob.glob(\"D:/FinalDphi/animal_dataset_intermediate/train/\"+class_name+\"_train/*\") # get files in each folder(class)\n",
    "        for f in files:\n",
    "            img = cv2.imread(f) #read the image\n",
    "            img = cv2.resize(img,(64,64)).flatten() #resize the image\n",
    "            training_data.append(np.array(img)) #Append images and corresponding labels to data\n",
    "            la=[0,0,0,0,0]\n",
    "            la[class_names.index(class_name)]=1\n",
    "            label.append(la)\n",
    "    except:\n",
    "        pass\n",
    "from sklearn.utils import shuffle\n",
    "training_data, label= shuffle(training_data, label, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([218, 182, 174, ...,  86, 133, 137], dtype=uint8)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Pre-processing\n",
    "For pre-processing I have converted into a numpy array and changed the type to float32 to be able to run the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(training_data) \n",
    "training_data = training_data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(label) \n",
    "label = label.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288,)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Training the model\n",
    "I used very generic values to train the model as first I was concerned about comparing the results of ANN to CNN. Training Validation split 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "103/102 [==============================] - 1s 7ms/step - loss: 11.4409 - accuracy: 0.2154 - val_loss: 7.7839 - val_accuracy: 0.2311\n",
      "Epoch 2/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 5.9533 - accuracy: 0.2299 - val_loss: 4.4820 - val_accuracy: 0.2311\n",
      "Epoch 3/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 3.6243 - accuracy: 0.2305 - val_loss: 2.9318 - val_accuracy: 0.2311\n",
      "Epoch 4/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 2.5450 - accuracy: 0.2302 - val_loss: 2.2197 - val_accuracy: 0.2311\n",
      "Epoch 5/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 2.0387 - accuracy: 0.2314 - val_loss: 1.9052 - val_accuracy: 0.2146\n",
      "Epoch 6/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.8257 - accuracy: 0.2312 - val_loss: 1.7607 - val_accuracy: 0.2311\n",
      "Epoch 7/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.7334 - accuracy: 0.2315 - val_loss: 1.6949 - val_accuracy: 0.2311\n",
      "Epoch 8/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6883 - accuracy: 0.2311 - val_loss: 1.6595 - val_accuracy: 0.2311\n",
      "Epoch 9/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6555 - accuracy: 0.2299 - val_loss: 1.6474 - val_accuracy: 0.2311\n",
      "Epoch 10/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6546 - accuracy: 0.2300 - val_loss: 1.6466 - val_accuracy: 0.2311\n",
      "Epoch 11/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6389 - accuracy: 0.2309 - val_loss: 1.6283 - val_accuracy: 0.2311\n",
      "Epoch 12/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6300 - accuracy: 0.2303 - val_loss: 1.6385 - val_accuracy: 0.2311\n",
      "Epoch 13/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6400 - accuracy: 0.2311 - val_loss: 1.6320 - val_accuracy: 0.2311\n",
      "Epoch 14/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6329 - accuracy: 0.2314 - val_loss: 1.6243 - val_accuracy: 0.2311\n",
      "Epoch 15/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6244 - accuracy: 0.2322 - val_loss: 1.6296 - val_accuracy: 0.2311\n",
      "Epoch 16/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6239 - accuracy: 0.2318 - val_loss: 1.6180 - val_accuracy: 0.2311\n",
      "Epoch 17/30\n",
      "103/102 [==============================] - 1s 7ms/step - loss: 1.6280 - accuracy: 0.2312 - val_loss: 1.6308 - val_accuracy: 0.2311\n",
      "Epoch 18/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6177 - accuracy: 0.2317 - val_loss: 1.6121 - val_accuracy: 0.2311\n",
      "Epoch 19/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6217 - accuracy: 0.2320 - val_loss: 1.6178 - val_accuracy: 0.2311\n",
      "Epoch 20/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6159 - accuracy: 0.2335 - val_loss: 1.6098 - val_accuracy: 0.2311\n",
      "Epoch 21/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6171 - accuracy: 0.2326 - val_loss: 1.6181 - val_accuracy: 0.2311\n",
      "Epoch 22/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6155 - accuracy: 0.2311 - val_loss: 1.6135 - val_accuracy: 0.2311\n",
      "Epoch 23/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6121 - accuracy: 0.2320 - val_loss: 1.6097 - val_accuracy: 0.2311\n",
      "Epoch 24/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6107 - accuracy: 0.2325 - val_loss: 1.6087 - val_accuracy: 0.2311\n",
      "Epoch 25/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6091 - accuracy: 0.2320 - val_loss: 1.6087 - val_accuracy: 0.2311\n",
      "Epoch 26/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6086 - accuracy: 0.2322 - val_loss: 1.6073 - val_accuracy: 0.2311\n",
      "Epoch 27/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6077 - accuracy: 0.2312 - val_loss: 1.6071 - val_accuracy: 0.2311\n",
      "Epoch 28/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6083 - accuracy: 0.2351 - val_loss: 1.6134 - val_accuracy: 0.2311\n",
      "Epoch 29/30\n",
      "103/102 [==============================] - 1s 5ms/step - loss: 1.6082 - accuracy: 0.2318 - val_loss: 1.6065 - val_accuracy: 0.2311\n",
      "Epoch 30/30\n",
      "103/102 [==============================] - 1s 6ms/step - loss: 1.6064 - accuracy: 0.2320 - val_loss: 1.6054 - val_accuracy: 0.2311\n"
     ]
    }
   ],
   "source": [
    "cl=classifier.fit(x=training_data,y=label, validation_split=0.2, batch_size = 64, epochs = 30,steps_per_epoch=len(training_data)*0.8/64,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6:Visualiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2683a72e988>]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(cl.history['loss'])\n",
    "plt.plot(cl.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 1s 3ms/step - loss: 1.6057 - accuracy: 0.2319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6057441234588623, 0.23194241523742676]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(x=training_data,y=label,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The glaringly conclusive result is that the model is not performing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Building a CNN\n",
    "We have used Sequential API which helps us define the model in a step by step manner. I tried increasing the input shape and it gave me a better result. Channel dimension is just to make sure that the RGB channels remain the last input in shape.\n",
    "\n",
    "Convolutional neural networks apply a filter to an input to create a feature map that summarizes the presence of detected features in the input. The Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting. In simple terms, we are just taking a small window and picking up samples from the image.\n",
    "\n",
    "Every Convulution is layer is followed by a layer for relu activation. The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. ReLU is important because it does not saturate; the gradient is always high (equal to 1) if the neuron activates. As long as it is not a dead neuron, successive updates are fairly effective. ReLU is also very quick to evaluate.\n",
    "\n",
    "Batch normalization is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling. Basically this is used for accelerating the whole process.\n",
    "\n",
    "The pooling operation involves sliding a two-dimensional filter over each channel of feature map and summarising the features lying within the region covered by the filter. It reducea the dimensions of the feature maps, thereby reducing the trainable parameters. As  further operations are performed on summarised features instead of precisely positioned features generated by the convolution layer, the model becomes more robust.\n",
    "###### The dense layer is given the value of 5 as we have 5 labels in our dataset\n",
    "###### Dropout is being used for optimization. It is a regularization technique in which we ommit units randomly to reduce the chances of overfitting of a model.\n",
    "Softmax is being used for the final activation layer as it “returns a probability distribution over the target classes in a multiclass classification problem” \n",
    "\n",
    "###### Note: The steps have been repeated in order to increase the model accuracy ( Later On)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "inputShape = (96, 96, 3)\n",
    "chanDim = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 96, 96, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 5125      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 8,678,277\n",
      "Trainable params: 8,675,397\n",
      "Non-trainable params: 2,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "from os import listdir\n",
    "import glob\n",
    "import random\n",
    "j=0\n",
    "for i in random.sample(glob.glob('D:/FinalDphi/animal_dataset_intermediate/train/*'),1152):\n",
    "    imageObject = Image.open(i);\n",
    "    imageObject=imageObject.convert('RGB')\n",
    "    g=imageObject.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    g.save('D:/FinalDphi/animal_dataset_intermediate/train/'+str(j)+'.jpg', 'JPEG')\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Loading Data  (2.0) \n",
    "Image Data Generator was used to load the data as it automatically creates the label (by defining classes) form the batches of perticular batch sizes, and automatically converts the images into the target size.\n",
    "Then to create a validation dataset I used random and glob library to select 10% images randomly from each category and move it to a new folder called Validation with similar label folders.\n",
    "Valid batches was also loaded using Image Data genertor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'D:/FinalDphi/animal_dataset_intermediate/train/'\n",
    "# validate_path='/Dataset COVID-19 Augmented/validate'\n",
    "# test_path = '/Dataset COVID-19 Augmented/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7377 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=train_path, target_size=(96,96), classes=['elefante_train', 'farfalla_train','mucca_train','pecora_train','scoiattolo_train'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rnd.sample(glob.glob('D:/FinalDphi/animal_dataset_intermediate/train/elefante_train/*'),130):\n",
    "    shutil.move(i,'D:/FinalDphi/animal_dataset_intermediate/validate/elefante_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rnd.sample(glob.glob('D:/FinalDphi/animal_dataset_intermediate/train/farfalla_train/*'),190):\n",
    "        shutil.move(i,'D:/FinalDphi/animal_dataset_intermediate/validate/farfalla_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rnd.sample(glob.glob('D:/FinalDphi/animal_dataset_intermediate/train/mucca_train/*'),168):\n",
    "        shutil.move(i,'D:/FinalDphi/animal_dataset_intermediate/validate/mucca_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rnd.sample(glob.glob('D:/FinalDphi/animal_dataset_intermediate/train/pecora_train/*'),164):\n",
    "        shutil.move(i,'D:/FinalDphi/animal_dataset_intermediate/validate/pecora_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rnd.sample(glob.glob('D:/FinalDphi/animal_dataset_intermediate/train/scoiattolo_train/*'),167):\n",
    "        shutil.move(i,'D:/FinalDphi/animal_dataset_intermediate/validate/scoiattolo_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_path='D:/FinalDphi/animal_dataset_intermediate/validate/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 819 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=validate_path, target_size=(96,96), classes=['elefante_train', 'farfalla_train','mucca_train','pecora_train','scoiattolo_train'], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Loss Function and Optimizer\n",
    "We have used adam for optimizaion once again so the explanation remains the same. \n",
    "\n",
    "Loss function being used here is called categorical crossentropy as it gave better results.\n",
    "Input_dimensions are based on model summarry and batch size remains the same( Used while loading the images).\n",
    "I trained it to 30 epochs just for comparison with ANN. The learning rate remained the same. I decreased the decay by a bit to reach the optimum value a bit quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 32\n",
    "IMAGE_DIMS = (96, 96, 3)\n",
    "INIT_LR = 1e-3\n",
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=30\n",
    "opt = Adam(lr=1e-3, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Training the model\n",
    "I used very generic values to train the model as first I was concerned about comparing the results of ANN to CNN. Training Validation split 90:10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "231/231 [==============================] - 21s 91ms/step - loss: 1.3330 - accuracy: 0.5539 - val_loss: 1.2722 - val_accuracy: 0.5568\n",
      "Epoch 2/30\n",
      "231/231 [==============================] - 25s 109ms/step - loss: 0.9631 - accuracy: 0.6485 - val_loss: 1.1248 - val_accuracy: 0.6129\n",
      "Epoch 3/30\n",
      "231/231 [==============================] - 29s 125ms/step - loss: 0.8166 - accuracy: 0.6993 - val_loss: 1.1108 - val_accuracy: 0.5873\n",
      "Epoch 4/30\n",
      "231/231 [==============================] - 24s 106ms/step - loss: 0.7169 - accuracy: 0.7346 - val_loss: 0.7925 - val_accuracy: 0.7387\n",
      "Epoch 5/30\n",
      "231/231 [==============================] - 25s 106ms/step - loss: 0.6464 - accuracy: 0.7647 - val_loss: 0.7382 - val_accuracy: 0.7570\n",
      "Epoch 6/30\n",
      "231/231 [==============================] - 24s 106ms/step - loss: 0.5817 - accuracy: 0.7885 - val_loss: 0.8016 - val_accuracy: 0.7216\n",
      "Epoch 7/30\n",
      "231/231 [==============================] - 24s 105ms/step - loss: 0.6090 - accuracy: 0.7742 - val_loss: 0.9151 - val_accuracy: 0.7228\n",
      "Epoch 8/30\n",
      "231/231 [==============================] - 24s 106ms/step - loss: 0.5031 - accuracy: 0.8137 - val_loss: 0.7479 - val_accuracy: 0.7375\n",
      "Epoch 9/30\n",
      "231/231 [==============================] - 28s 121ms/step - loss: 0.4687 - accuracy: 0.8281 - val_loss: 0.6356 - val_accuracy: 0.7753\n",
      "Epoch 10/30\n",
      "231/231 [==============================] - 33s 141ms/step - loss: 0.4764 - accuracy: 0.8254 - val_loss: 0.7132 - val_accuracy: 0.7619\n",
      "Epoch 11/30\n",
      "231/231 [==============================] - 28s 122ms/step - loss: 0.3918 - accuracy: 0.8525 - val_loss: 0.8036 - val_accuracy: 0.7302\n",
      "Epoch 12/30\n",
      "231/231 [==============================] - 24s 105ms/step - loss: 0.3745 - accuracy: 0.8639 - val_loss: 0.9424 - val_accuracy: 0.7289\n",
      "Epoch 13/30\n",
      "231/231 [==============================] - 30s 129ms/step - loss: 0.3118 - accuracy: 0.8818 - val_loss: 0.6326 - val_accuracy: 0.7985\n",
      "Epoch 14/30\n",
      "231/231 [==============================] - 24s 106ms/step - loss: 0.2717 - accuracy: 0.8993 - val_loss: 0.7373 - val_accuracy: 0.7839\n",
      "Epoch 15/30\n",
      "231/231 [==============================] - 35s 150ms/step - loss: 0.2529 - accuracy: 0.9063 - val_loss: 0.7765 - val_accuracy: 0.7753\n",
      "Epoch 16/30\n",
      "231/231 [==============================] - 24s 106ms/step - loss: 0.2364 - accuracy: 0.9143 - val_loss: 0.5735 - val_accuracy: 0.8230\n",
      "Epoch 17/30\n",
      "231/231 [==============================] - 24s 106ms/step - loss: 0.1971 - accuracy: 0.9284 - val_loss: 0.6083 - val_accuracy: 0.8266\n",
      "Epoch 18/30\n",
      "231/231 [==============================] - 32s 140ms/step - loss: 0.1974 - accuracy: 0.9287 - val_loss: 0.7930 - val_accuracy: 0.7766\n",
      "Epoch 19/30\n",
      "231/231 [==============================] - 25s 107ms/step - loss: 0.1989 - accuracy: 0.9282 - val_loss: 0.7572 - val_accuracy: 0.7778\n",
      "Epoch 20/30\n",
      "231/231 [==============================] - 26s 114ms/step - loss: 0.1669 - accuracy: 0.9381 - val_loss: 0.7630 - val_accuracy: 0.7985\n",
      "Epoch 21/30\n",
      "231/231 [==============================] - 24s 105ms/step - loss: 0.1540 - accuracy: 0.9429 - val_loss: 0.7390 - val_accuracy: 0.7924\n",
      "Epoch 22/30\n",
      "231/231 [==============================] - 25s 108ms/step - loss: 0.1465 - accuracy: 0.9465 - val_loss: 0.7881 - val_accuracy: 0.8156\n",
      "Epoch 23/30\n",
      "231/231 [==============================] - 17s 72ms/step - loss: 0.1408 - accuracy: 0.9505 - val_loss: 0.9984 - val_accuracy: 0.7558\n",
      "Epoch 24/30\n",
      "231/231 [==============================] - 12s 51ms/step - loss: 0.1809 - accuracy: 0.9356 - val_loss: 0.8668 - val_accuracy: 0.7790\n",
      "Epoch 25/30\n",
      "231/231 [==============================] - 12s 51ms/step - loss: 0.2781 - accuracy: 0.9055 - val_loss: 0.8198 - val_accuracy: 0.8144\n",
      "Epoch 26/30\n",
      "231/231 [==============================] - 12s 50ms/step - loss: 0.1696 - accuracy: 0.9362 - val_loss: 0.8084 - val_accuracy: 0.7888\n",
      "Epoch 27/30\n",
      "231/231 [==============================] - 12s 51ms/step - loss: 0.1426 - accuracy: 0.9482 - val_loss: 0.7281 - val_accuracy: 0.8046\n",
      "Epoch 28/30\n",
      "231/231 [==============================] - 12s 50ms/step - loss: 0.0934 - accuracy: 0.9672 - val_loss: 0.6117 - val_accuracy: 0.8400\n",
      "Epoch 29/30\n",
      "231/231 [==============================] - 12s 50ms/step - loss: 0.0746 - accuracy: 0.9729 - val_loss: 0.6871 - val_accuracy: 0.8303\n",
      "Epoch 30/30\n",
      "231/231 [==============================] - 12s 50ms/step - loss: 0.0844 - accuracy: 0.9687 - val_loss: 0.7494 - val_accuracy: 0.8168\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "H = model.fit(x=train_batches, batch_size=32,validation_data=valid_batches,steps_per_epoch=len(train_batches), epochs= 30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = model.fit(x=train_batches, batch_size=32,validation_data=valid_batches,steps_per_epoch=len(train_batches), epochs= 30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6:Visualiztion (2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26f7e29f988>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(H.history['loss'])\n",
    "plt.plot(H.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"D:/FinalDphi/mod.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 10s 43ms/step - loss: 0.0062 - accuracy: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.006182956974953413, 0.999186635017395]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_batches,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Prediction\n",
    "The test_data was re-ordered into the csv format by using PIL library. Now our test_data lies in a new target called test_new. I used data generator to load the images in batches. \n",
    "##### Once I received the predictions I used argmax(because we are using softmax) to convert it into integer values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('D:/FinalDphi/animal_dataset_intermediate/Testing_set_animals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e030b20928e90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e030b20929e90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e030b2092be90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e030b2092ce90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e030b2092de90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  target\n",
       "0  e030b20928e90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "1  e030b20929e90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "2  e030b2092be90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "3  e030b2092ce90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "4  e030b2092de90021d85a5854ee454296eb70e3c818b413...     NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "for i in range(len(test)):\n",
    "    imgo = Image.open('D:/FinalDphi/animal_dataset_intermediate/test/'+str(test['filename'][i]))\n",
    "    imgo=imgo.convert('RGB')\n",
    "    imgo.save('D:/FinalDphi/animal_dataset_intermediate/new_test/'+str(i)+'.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 910 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\"D:/FinalDphi/animal_dataset_intermediate/\",\n",
    "                                                    batch_size = 32,\n",
    "                                                    classes=['new_test'], \n",
    "                                                    target_size = (96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_generator) \n",
    "predicted_labels  = np.argmax(preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 1, 3, 3, 4, 3, 2, 1, 1, 4, 1, 2, 3, 2, 1, 3, 3, 3, 1, 1, 2,\n",
       "       1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3,\n",
       "       1, 1, 3, 3, 4, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 3, 3, 2, 1, 3, 1, 3,\n",
       "       3, 1, 3, 3, 4, 1, 4, 3, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 3, 1, 2, 3,\n",
       "       1, 2, 1, 3, 1, 2, 3, 1, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 0,\n",
       "       4, 3, 1, 4, 4, 3, 3, 2, 3, 3, 1, 3, 2, 3, 1, 3, 1, 1, 2, 3, 3, 3,\n",
       "       2, 1, 1, 1, 1, 4, 1, 1, 3, 1, 3, 1, 1, 4, 1, 3, 1, 4, 3, 2, 1, 3,\n",
       "       1, 3, 1, 3, 3, 4, 1, 1, 2, 1, 3, 4, 1, 1, 3, 3, 3, 1, 3, 1, 1, 1,\n",
       "       3, 3, 2, 1, 2, 3, 3, 3, 1, 3, 0, 1, 3, 3, 1, 1, 4, 3, 1, 3, 0, 1,\n",
       "       3, 2, 4, 3, 1, 2, 3, 3, 1, 2, 1, 1, 3, 2, 3, 3, 2, 3, 1, 4, 3, 3,\n",
       "       1, 3, 3, 4, 3, 3, 1, 4, 3, 3, 1, 1, 4, 2, 2, 2, 3, 4, 1, 1, 3, 2,\n",
       "       3, 1, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1, 2, 3, 2, 1, 3, 1, 3, 1,\n",
       "       1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 2, 3, 1, 3, 4, 2, 1, 3, 3, 3, 1, 3,\n",
       "       4, 3, 4, 3, 3, 3, 3, 2, 3, 1, 3, 3, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3,\n",
       "       1, 3, 1, 1, 3, 1, 1, 2, 2, 3, 1, 3, 3, 4, 3, 3, 3, 1, 3, 1, 1, 2,\n",
       "       1, 1, 3, 3, 3, 2, 1, 3, 1, 2, 1, 3, 2, 1, 1, 4, 1, 1, 1, 1, 2, 1,\n",
       "       1, 4, 1, 3, 2, 3, 3, 3, 3, 2, 1, 1, 2, 1, 2, 3, 1, 4, 1, 3, 1, 3,\n",
       "       3, 4, 3, 1, 2, 2, 3, 1, 3, 3, 2, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1,\n",
       "       1, 3, 1, 3, 3, 3, 3, 1, 4, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 1, 4,\n",
       "       3, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1, 3, 3, 3,\n",
       "       3, 1, 3, 2, 3, 3, 3, 3, 3, 1, 4, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3,\n",
       "       2, 3, 2, 1, 3, 3, 3, 1, 1, 4, 3, 2, 3, 1, 3, 1, 1, 1, 1, 3, 1, 1,\n",
       "       1, 3, 3, 1, 3, 3, 4, 3, 1, 3, 3, 3, 4, 1, 3, 3, 1, 3, 1, 3, 1, 3,\n",
       "       3, 1, 3, 1, 3, 3, 3, 4, 1, 1, 4, 3, 3, 3, 2, 1, 2, 2, 1, 2, 3, 3,\n",
       "       3, 3, 2, 3, 4, 3, 4, 3, 3, 3, 2, 3, 1, 3, 3, 1, 3, 3, 2, 1, 3, 1,\n",
       "       1, 2, 3, 3, 3, 3, 3, 2, 1, 3, 1, 4, 2, 3, 1, 3, 3, 3, 3, 3, 1, 3,\n",
       "       3, 4, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 2, 3, 3, 3, 3, 3, 3, 4, 0,\n",
       "       1, 1, 2, 3, 2, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3,\n",
       "       1, 1, 3, 1, 1, 4, 4, 1, 3, 1, 1, 1, 1, 3, 1, 4, 1, 2, 4, 1, 3, 1,\n",
       "       3, 1, 3, 1, 3, 1, 3, 1, 1, 2, 4, 3, 4, 1, 3, 1, 3, 4, 1, 1, 3, 4,\n",
       "       3, 3, 1, 1, 3, 3, 3, 1, 4, 3, 1, 1, 3, 1, 1, 3, 1, 2, 1, 3, 1, 3,\n",
       "       1, 3, 1, 3, 3, 2, 3, 4, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 4, 1, 3, 3,\n",
       "       3, 1, 2, 3, 1, 1, 4, 3, 1, 3, 4, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1,\n",
       "       4, 1, 2, 3, 3, 1, 1, 2, 3, 1, 3, 1, 1, 1, 3, 2, 3, 3, 2, 2, 1, 3,\n",
       "       2, 3, 2, 1, 3, 4, 2, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 4, 1, 1, 3, 2,\n",
       "       3, 1, 1, 4, 4, 3, 1, 1, 0, 3, 1, 4, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1,\n",
       "       4, 1, 1, 2, 3, 4, 1, 1, 3, 3, 1, 4, 3, 1, 1, 3, 1, 3, 3, 3, 3, 1,\n",
       "       3, 3, 3, 1, 4, 1, 3, 3, 2, 3, 1, 3, 1, 3, 3, 1, 3, 1, 3, 1, 3, 3,\n",
       "       2, 1, 4, 3, 3, 3, 2, 2, 2, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3, 3, 1, 4,\n",
       "       3, 3, 1, 3, 4, 3, 3, 2, 3, 1, 3, 3, 3, 2, 1, 1, 2, 1, 1, 3, 1, 3,\n",
       "       3, 3, 1, 3, 3, 3, 3, 3, 1, 0, 1, 1, 2, 1, 3, 3, 1, 0, 3, 1, 3, 3,\n",
       "       1, 3, 1, 1, 2, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=['elefante', 'farfalla','mucca','pecora','scoiattolo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values converted into label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values=[cl[i] for i in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = (train_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elefante_train': 0,\n",
       " 'farfalla_train': 1,\n",
       " 'mucca_train': 2,\n",
       " 'pecora_train': 3,\n",
       " 'scoiattolo_train': 4}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'elefante',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'elefante',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'elefante',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'mucca',\n",
       " 'mucca',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'elefante',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'elefante',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'mucca',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'scoiattolo',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'elefante',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'elefante',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'pecora',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'mucca',\n",
       " 'farfalla',\n",
       " 'farfalla',\n",
       " 'farfalla']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scoiattolo', 'pecora', 'farfalla', 'pecora']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_values[490:494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(predicted_values) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\n",
    "res.columns = [\"prediction\"]\n",
    "res.to_csv(\"prediction_Assignment2_101.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying for ages and receiving and receiving 97% train accuracy and 81% validation accuracy I figured out that the results werent satisfactory for me. The difference between train and validation was a clear sign of overfitting and the result on dphi wasnt that good so I researched a bit and came up with this solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Building The final model\n",
    "I used VGG-16 to train the the model.VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014.\n",
    "This can be classified as deep transfered learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Loading Data  Final\n",
    "Through a bit of research I found this way to load data using csv. I found it interesting and useful considering we need to reoreder images for predictions. Basically we are doing the same process of saving the data by going through each foder and assigning the label but we are saving the data in a csv instead of saving them in a list. I also split the data further into train and validation sets but ended up not using the split and going with th e whole dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>D:/FinalDphi/animal_dataset_intermediate/train...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filename category\n",
       "0     D:/FinalDphi/animal_dataset_intermediate/train...        0\n",
       "1     D:/FinalDphi/animal_dataset_intermediate/train...        0\n",
       "2     D:/FinalDphi/animal_dataset_intermediate/train...        0\n",
       "3     D:/FinalDphi/animal_dataset_intermediate/train...        0\n",
       "4     D:/FinalDphi/animal_dataset_intermediate/train...        0\n",
       "...                                                 ...      ...\n",
       "2495  D:/FinalDphi/animal_dataset_intermediate/train...        4\n",
       "2496  D:/FinalDphi/animal_dataset_intermediate/train...        4\n",
       "2497  D:/FinalDphi/animal_dataset_intermediate/train...        4\n",
       "2498  D:/FinalDphi/animal_dataset_intermediate/train...        4\n",
       "2499  D:/FinalDphi/animal_dataset_intermediate/train...        4\n",
       "\n",
       "[2500 rows x 2 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldernames = os.listdir('D:/FinalDphi/animal_dataset_intermediate/train')\n",
    "categories = []\n",
    "files = []\n",
    "i = 0\n",
    "for k, folder in enumerate(foldernames):\n",
    "    filenames = os.listdir(\"D:/FinalDphi/animal_dataset_intermediate/train/\" + folder);\n",
    "    for file in filenames:\n",
    "        files.append(\"D:/FinalDphi/animal_dataset_intermediate/train/\" + folder + \"/\" + file)\n",
    "        categories.append(k)\n",
    "        \n",
    "df = pd.DataFrame({\n",
    "    'filename': files,\n",
    "    'category': categories\n",
    "})\n",
    "train_df = pd.DataFrame(columns=['filename', 'category'])\n",
    "for i in range(10):\n",
    "    train_df = train_df.append(df[df.category == i].iloc[:500,:])\n",
    "\n",
    "train_df.head()\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8196"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['filename']=='D:/FinalDphi/animal_dataset_intermediate/train/elefante_train/filenames_elefante_train.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['filename']\n",
    "y = df['category']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Pre-procesing Optimization Techniques\n",
    "I have previously talked about decay and applying dropout for optimization. This time i used image centering as well. At the core of it, image centering is just a scaling technique but it lead to better results. The process simply involves calculating the mean pixel value across the entire training dataset, then subtract it from each image.  The mean pixel value for centered images will be zero.\n",
    "\n",
    "I used TQDM just to ensure that the process was working well. it is a progress bar library with good support for nested loops and Jupyter/IPython notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Predator\\anaconda3\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927f1c40665c4467b632de30b4adca79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8196.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def centering_image(img):\n",
    "    size = [256,256]\n",
    "    \n",
    "    img_size = img.shape[:2]\n",
    "    \n",
    "    # centering\n",
    "    row = (size[1] - img_size[0]) // 2\n",
    "    col = (size[0] - img_size[1]) // 2\n",
    "    resized = np.zeros(list(size) + [img.shape[2]], dtype=np.uint8)\n",
    "    resized[row:(row + img.shape[0]), col:(col + img.shape[1])] = img\n",
    "\n",
    "    return resized\n",
    "\n",
    "images = []\n",
    "with tqdm(total=len(df)) as pbar:\n",
    "    for i, file_path in enumerate(df.filename.values):\n",
    "        #read image\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#resize\n",
    "        if(img.shape[0] > img.shape[1]):\n",
    "            tile_size = (int(img.shape[1]*256/img.shape[0]),256)\n",
    "        else:\n",
    "            tile_size = (256, int(img.shape[0]*256/img.shape[1]))\n",
    "\n",
    "        #centering\n",
    "        img = centering_image(cv2.resize(img, dsize=tile_size))\n",
    "\n",
    "        #out put 224*224px \n",
    "        img = img[16:240, 16:240]\n",
    "        images.append(img)\n",
    "        pbar.update(1)\n",
    "\n",
    "images = np.array(images)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to increase the dimenstions to 224 as increasing dimenstions was improving the result. The code snipped bellow just shuffles the data and splits into train and validation sets(80:20). \n",
    "###### It also converts data into train and validations splits for training\n",
    "###### Further scaling is done through dividing it by 255 and finally it has been converted into a numpy array of float for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (6557, 224, 224, 3)\n",
      "y_train (6557,)\n",
      "x_test (1639, 224, 224, 3)\n",
      "y_test (1639,)\n"
     ]
    }
   ],
   "source": [
    "data_num = len(y)\n",
    "random_index = np.random.permutation(data_num)\n",
    "x_shuffle = []\n",
    "y_shuffle = []\n",
    "for i in range(data_num):\n",
    "    x_shuffle.append(images[random_index[i]])\n",
    "    y_shuffle.append(y[random_index[i]])\n",
    "    \n",
    "x = np.array(x_shuffle) \n",
    "y = np.array(y_shuffle)\n",
    "val_split_num = int(round(0.2*len(y)))\n",
    "x_train = x[val_split_num:]\n",
    "y_train = y[val_split_num:]\n",
    "x_test = x[:val_split_num]\n",
    "y_test = y[:val_split_num]\n",
    "\n",
    "print('x_train', x_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('x_test', x_test.shape)\n",
    "print('y_test', y_test.shape)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "img_rows, img_cols, img_channel = 224, 224, 3\n",
    "name_animal = []\n",
    "for i in range(5):\n",
    "    path = df[df.category == i].values[2]\n",
    "    if path[0].split('/')[-2] == 'elefante':\n",
    "        name_animal.append('elefante')\n",
    "    elif path[0].split('/')[-2] == 'farfalla':\n",
    "        name_animal.append('farfalla')\n",
    "    elif path[0].split('/')[-2] == 'mucca':\n",
    "        name_animal.append('mucca')\n",
    "    elif path[0].split('/')[-2] == 'pecora':\n",
    "        name_animal.append('pecora')\n",
    "    elif path[0].split('/')[-2] == 'scoiattolo':\n",
    "        name_animal.append('scoiattolo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Model Summary, Loss Function and Optimizer \n",
    "The base of the modell= is vgg.16 and then using sequential apu two dense layer have been added with the final layer having 5 units to mark the 5 categories. The use of relu and softmax has been discused previously. I changed the optimizer to SGD. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient by an estimate thereof. In simpler terms, we define a momentum, which is a moving average of our gradients. I found this approach very flexible.\n",
    "\n",
    "Loss function being used here is called binary crossentropy. It is generally used for classification as it uses probability distribution to give us a 1 or 0 value. \n",
    "###### I did not use categorical crossentropy as labels are already one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (None, 5)                 6424069   \n",
      "=================================================================\n",
      "Total params: 21,138,757\n",
      "Trainable params: 21,138,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "add_model = Sequential()\n",
    "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "add_model.add(Dense(256, activation='relu'))\n",
    "add_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization through image augmentation\n",
    "Augmentation is just varying our current dataset in diffent formats( like flipping/moving) and appenfing it to the dataset to increase the dataset and improve its perfirmance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=30, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1, \n",
    "        horizontal_flip=True)\n",
    "train_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I learned about checkpoints. Checkpoint moniters the validation accuracy( as validation is more reliable and accuracy is our metric) and stops the model once we reach an optimal point after which the model starts to overfit. \n",
    "###### My model ran for the full 30 epochs so I increased it to 50 to get a converging value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  2/204 [..............................] - ETA: 57s - loss: 0.5064 - accuracy: 0.2031WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1656s vs `on_train_batch_end` time: 0.3820s). Check your callbacks.\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.4996WARNING:tensorflow:From C:\\Users\\Predator\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Predator\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 645ms/step - loss: 0.4061 - accuracy: 0.4996 - val_loss: 0.2785 - val_accuracy: 0.6986\n",
      "Epoch 2/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.7413INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 130s 637ms/step - loss: 0.2330 - accuracy: 0.7413 - val_loss: 0.1847 - val_accuracy: 0.7974\n",
      "Epoch 3/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.8179INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 630ms/step - loss: 0.1694 - accuracy: 0.8179 - val_loss: 0.1395 - val_accuracy: 0.8505\n",
      "Epoch 4/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.8625INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 130s 637ms/step - loss: 0.1296 - accuracy: 0.8625 - val_loss: 0.1199 - val_accuracy: 0.8719\n",
      "Epoch 5/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.8846INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 135s 661ms/step - loss: 0.1138 - accuracy: 0.8846 - val_loss: 0.0921 - val_accuracy: 0.9060\n",
      "Epoch 6/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9045INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 136s 669ms/step - loss: 0.0965 - accuracy: 0.9045 - val_loss: 0.0848 - val_accuracy: 0.9158\n",
      "Epoch 7/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9146INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 136s 666ms/step - loss: 0.0869 - accuracy: 0.9146 - val_loss: 0.0904 - val_accuracy: 0.9097\n",
      "Epoch 8/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9212INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 137s 671ms/step - loss: 0.0802 - accuracy: 0.9212 - val_loss: 0.0732 - val_accuracy: 0.9256\n",
      "Epoch 9/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9267INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 134s 655ms/step - loss: 0.0742 - accuracy: 0.9267 - val_loss: 0.0806 - val_accuracy: 0.9207\n",
      "Epoch 10/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9347INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 141s 690ms/step - loss: 0.0676 - accuracy: 0.9347 - val_loss: 0.0645 - val_accuracy: 0.9317\n",
      "Epoch 11/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9418INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 136s 667ms/step - loss: 0.0626 - accuracy: 0.9418 - val_loss: 0.0622 - val_accuracy: 0.9329\n",
      "Epoch 12/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9439INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 138s 675ms/step - loss: 0.0596 - accuracy: 0.9439 - val_loss: 0.0597 - val_accuracy: 0.9384\n",
      "Epoch 13/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9474INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 135s 664ms/step - loss: 0.0542 - accuracy: 0.9474 - val_loss: 0.0600 - val_accuracy: 0.9347\n",
      "Epoch 14/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9514INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 642ms/step - loss: 0.0518 - accuracy: 0.9514 - val_loss: 0.0732 - val_accuracy: 0.9274\n",
      "Epoch 15/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9479INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 133s 650ms/step - loss: 0.0512 - accuracy: 0.9479 - val_loss: 0.0494 - val_accuracy: 0.9518\n",
      "Epoch 16/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9572INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 132s 646ms/step - loss: 0.0465 - accuracy: 0.9572 - val_loss: 0.0497 - val_accuracy: 0.9457\n",
      "Epoch 17/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9563INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 130s 639ms/step - loss: 0.0441 - accuracy: 0.9563 - val_loss: 0.0537 - val_accuracy: 0.9451\n",
      "Epoch 18/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9571INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 633ms/step - loss: 0.0447 - accuracy: 0.9571 - val_loss: 0.0490 - val_accuracy: 0.9469\n",
      "Epoch 19/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9658INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 128s 628ms/step - loss: 0.0377 - accuracy: 0.9658 - val_loss: 0.0467 - val_accuracy: 0.9500\n",
      "Epoch 20/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9652INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 130s 637ms/step - loss: 0.0380 - accuracy: 0.9652 - val_loss: 0.0563 - val_accuracy: 0.9420\n",
      "Epoch 21/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9654INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 634ms/step - loss: 0.0377 - accuracy: 0.9654 - val_loss: 0.0502 - val_accuracy: 0.9481\n",
      "Epoch 22/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9686INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 127s 624ms/step - loss: 0.0357 - accuracy: 0.9686 - val_loss: 0.0449 - val_accuracy: 0.9524\n",
      "Epoch 23/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9683INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 631ms/step - loss: 0.0340 - accuracy: 0.9683 - val_loss: 0.0468 - val_accuracy: 0.9512\n",
      "Epoch 24/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9713INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 127s 625ms/step - loss: 0.0326 - accuracy: 0.9713 - val_loss: 0.0501 - val_accuracy: 0.9530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9718INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 633ms/step - loss: 0.0313 - accuracy: 0.9718 - val_loss: 0.0530 - val_accuracy: 0.9475\n",
      "Epoch 26/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9693INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 631ms/step - loss: 0.0316 - accuracy: 0.9693 - val_loss: 0.0500 - val_accuracy: 0.9494\n",
      "Epoch 27/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9769INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 128s 627ms/step - loss: 0.0275 - accuracy: 0.9769 - val_loss: 0.0487 - val_accuracy: 0.9536\n",
      "Epoch 28/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9770INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 634ms/step - loss: 0.0249 - accuracy: 0.9770 - val_loss: 0.0503 - val_accuracy: 0.9530\n",
      "Epoch 29/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9781INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 130s 636ms/step - loss: 0.0260 - accuracy: 0.9781 - val_loss: 0.0461 - val_accuracy: 0.9542\n",
      "Epoch 30/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9808INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 128s 626ms/step - loss: 0.0235 - accuracy: 0.9808 - val_loss: 0.0445 - val_accuracy: 0.9573\n",
      "Epoch 31/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9785INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 130s 636ms/step - loss: 0.0248 - accuracy: 0.9785 - val_loss: 0.0536 - val_accuracy: 0.9500\n",
      "Epoch 32/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9770INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 128s 628ms/step - loss: 0.0265 - accuracy: 0.9770 - val_loss: 0.0428 - val_accuracy: 0.9585\n",
      "Epoch 33/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9810INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 641ms/step - loss: 0.0226 - accuracy: 0.9810 - val_loss: 0.0444 - val_accuracy: 0.9573\n",
      "Epoch 34/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9811INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 130s 640ms/step - loss: 0.0218 - accuracy: 0.9811 - val_loss: 0.0452 - val_accuracy: 0.9555\n",
      "Epoch 35/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9838INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 635ms/step - loss: 0.0187 - accuracy: 0.9838 - val_loss: 0.0430 - val_accuracy: 0.9616\n",
      "Epoch 36/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9828INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 631ms/step - loss: 0.0187 - accuracy: 0.9828 - val_loss: 0.0493 - val_accuracy: 0.9561\n",
      "Epoch 37/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9839INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 642ms/step - loss: 0.0187 - accuracy: 0.9839 - val_loss: 0.0462 - val_accuracy: 0.9573\n",
      "Epoch 38/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9847INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 128s 627ms/step - loss: 0.0171 - accuracy: 0.9847 - val_loss: 0.0446 - val_accuracy: 0.9634\n",
      "Epoch 39/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9833INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 634ms/step - loss: 0.0189 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9585\n",
      "Epoch 40/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9851INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 133s 650ms/step - loss: 0.0172 - accuracy: 0.9851 - val_loss: 0.0604 - val_accuracy: 0.9469\n",
      "Epoch 41/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9828INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 130s 636ms/step - loss: 0.0192 - accuracy: 0.9828 - val_loss: 0.0460 - val_accuracy: 0.9610\n",
      "Epoch 42/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9870INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 641ms/step - loss: 0.0147 - accuracy: 0.9870 - val_loss: 0.0505 - val_accuracy: 0.9500\n",
      "Epoch 43/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9877INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 135s 661ms/step - loss: 0.0141 - accuracy: 0.9877 - val_loss: 0.0483 - val_accuracy: 0.9616\n",
      "Epoch 44/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9880INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 129s 634ms/step - loss: 0.0154 - accuracy: 0.9880 - val_loss: 0.0453 - val_accuracy: 0.9610\n",
      "Epoch 45/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9879INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 644ms/step - loss: 0.0142 - accuracy: 0.9879 - val_loss: 0.0463 - val_accuracy: 0.9603\n",
      "Epoch 46/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9897INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 642ms/step - loss: 0.0128 - accuracy: 0.9897 - val_loss: 0.0631 - val_accuracy: 0.9506\n",
      "Epoch 47/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9893INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 641ms/step - loss: 0.0124 - accuracy: 0.9893 - val_loss: 0.0496 - val_accuracy: 0.9585\n",
      "Epoch 48/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9888INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 133s 654ms/step - loss: 0.0128 - accuracy: 0.9888 - val_loss: 0.0511 - val_accuracy: 0.9555\n",
      "Epoch 49/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9893INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 131s 640ms/step - loss: 0.0134 - accuracy: 0.9893 - val_loss: 0.0428 - val_accuracy: 0.9603\n",
      "Epoch 50/50\n",
      "204/204 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9903INFO:tensorflow:Assets written to: VGG16-transferlearning.model\\assets\n",
      "204/204 [==============================] - 132s 647ms/step - loss: 0.0112 - accuracy: 0.9903 - val_loss: 0.0496 - val_accuracy: 0.9591\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x272ca993ec8>]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 34s 165ms/step - loss: 0.0065 - accuracy: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.006494170054793358, 0.9942046403884888]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train,y_train,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 165ms/step - loss: 0.0496 - accuracy: 0.9591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04964476451277733, 0.9591214060783386]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"D:/FinalDphi/model2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "I was pretty Happy with the eavluations so I moved on to predictions.\n",
    "Using the same data Loading technique I was able to load the data in the desired format (mentioned in csv). I did the same preprocessing steps of centering and Rescaling the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('D:/FinalDphi/animal_dataset_intermediate/Testing_set_animals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e030b20928e90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e030b20929e90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e030b2092be90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e030b2092ce90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e030b2092de90021d85a5854ee454296eb70e3c818b413...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>OIP-5ual0F5ZPZRdkGcj8uSH0AHaFj.jpeg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>OIP-5VwIBS0B8SxZbUiAHBJg7gHaE8.jpeg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>OIP-5WG0rHWAZYtu0utoZfuaAgHaFj.jpeg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>OIP-5ZwfeYunG6CT2wEI7OjybQHaEo.jpeg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>OIP-6A6SpPbz_YUI4ElMo-UhuQHaE8.jpeg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>910 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  target\n",
       "0    e030b20928e90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "1    e030b20929e90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "2    e030b2092be90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "3    e030b2092ce90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "4    e030b2092de90021d85a5854ee454296eb70e3c818b413...     NaN\n",
       "..                                                 ...     ...\n",
       "905                OIP-5ual0F5ZPZRdkGcj8uSH0AHaFj.jpeg     NaN\n",
       "906                OIP-5VwIBS0B8SxZbUiAHBJg7gHaE8.jpeg     NaN\n",
       "907                OIP-5WG0rHWAZYtu0utoZfuaAgHaFj.jpeg     NaN\n",
       "908                OIP-5ZwfeYunG6CT2wEI7OjybQHaEo.jpeg     NaN\n",
       "909                OIP-6A6SpPbz_YUI4ElMo-UhuQHaE8.jpeg     NaN\n",
       "\n",
       "[910 rows x 2 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = \"D:/FinalDphi/animal_dataset_intermediate/test/\"+test_data['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "1      D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "2      D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "3      D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "4      D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "                             ...                        \n",
       "905    D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "906    D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "907    D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "908    D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "909    D:/FinalDphi/animal_dataset_intermediate/test/...\n",
       "Name: filename, Length: 910, dtype: object"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centering and rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Predator\\anaconda3\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6409156114450923914e911cae07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=910.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def centering_image(img):\n",
    "    size = [256,256]\n",
    "    \n",
    "    img_size = img.shape[:2]\n",
    "    \n",
    "    # centering\n",
    "    row = (size[1] - img_size[0]) // 2\n",
    "    col = (size[0] - img_size[1]) // 2\n",
    "    resized = np.zeros(list(size) + [img.shape[2]], dtype=np.uint8)\n",
    "    resized[row:(row + img.shape[0]), col:(col + img.shape[1])] = img\n",
    "\n",
    "    return resized\n",
    "\n",
    "test_d = []\n",
    "with tqdm(total=len(test_data)) as pbar:\n",
    "    for i, file_path in enumerate(test_data.filename.values):\n",
    "        #read image\n",
    "        img = cv2.imread(\"D:/FinalDphi/animal_dataset_intermediate/test/\"+file_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#resize\n",
    "        if(img.shape[0] > img.shape[1]):\n",
    "            tile_size = (int(img.shape[1]*256/img.shape[0]),256)\n",
    "        else:\n",
    "            tile_size = (256, int(img.shape[0]*256/img.shape[1]))\n",
    "\n",
    "        #centering\n",
    "        img = centering_image(cv2.resize(img, dsize=tile_size))\n",
    "\n",
    "        #out put 224*224px \n",
    "        img = img[16:240, 16:240]\n",
    "        test_d.append(img)\n",
    "        pbar.update(1)\n",
    "\n",
    "test_d = np.array(test_d)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n = len(x_test)\n",
    "testing=[]\n",
    "for i in range(data_n):\n",
    "    testing.append(test_d[i])\n",
    "    \n",
    "testing = np.array(testing) \n",
    "testing = testing.astype('float32')\n",
    "testing /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(testing)\n",
    "#if using softmax activation on output layer\n",
    "predicted_labels  = np.argmax(preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 3, 3, 3, 3, 0, 0, 0, 3, 1, 0, 0, 3, 1, 0, 0,\n",
       "       3, 3, 3, 3, 0, 1, 1, 1, 3, 3, 3, 3, 3, 0, 3, 1, 3, 1, 1, 3, 1, 3,\n",
       "       3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 3, 0, 1, 0, 3, 1, 1, 1, 3, 1,\n",
       "       3, 1, 1, 3, 0, 1, 3, 3, 3, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 3, 0,\n",
       "       0, 0, 1, 3, 0, 0, 0, 3, 1, 0, 3, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 3, 0, 1, 0, 3, 3, 0, 0, 3, 0, 1, 3, 0, 0, 0, 0, 3, 0, 1, 4, 3,\n",
       "       0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 1, 1, 0, 0, 0, 3, 1, 1,\n",
       "       0, 3, 1, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 3, 1, 0, 1,\n",
       "       3, 1, 1, 3, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 3,\n",
       "       3, 2, 0, 0, 3, 0, 4, 0, 1, 1, 1, 0, 1, 1, 1, 3, 1, 1, 1, 1, 1, 0,\n",
       "       3, 0, 1, 3, 1, 1, 0, 0, 3, 3, 0, 0, 0, 3, 0, 1, 1, 3, 0, 1, 1, 1,\n",
       "       0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 0, 1, 0, 3, 1, 1, 0, 1, 3, 0, 3, 0,\n",
       "       3, 3, 1, 1, 0, 1, 1, 1, 3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 3,\n",
       "       3, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 0, 0, 1, 3, 3, 0, 0,\n",
       "       0, 0, 0, 1, 3, 3, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 3, 1, 0, 3, 0, 3,\n",
       "       0, 0, 0, 0, 3, 3, 3, 3, 1, 0, 4, 3, 0, 3, 3, 1, 3, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 3, 4, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 3, 3, 0, 3, 3, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       3, 3, 3, 1, 1, 4, 0, 3, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3,\n",
       "       3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 3, 0, 3, 3, 1, 1, 1, 1, 3, 1,\n",
       "       1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 4,\n",
       "       4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 0, 2, 2, 4, 2, 2, 4, 2, 4, 4,\n",
       "       4, 2, 2, 2, 4, 2, 4, 3, 4, 2, 1, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2,\n",
       "       4, 4, 4, 4, 1, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 3, 2, 4, 2, 2,\n",
       "       1, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 3, 2, 2, 4, 2,\n",
       "       0, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4,\n",
       "       2, 2, 4, 4, 0, 0, 2, 2, 0, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2,\n",
       "       4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 2,\n",
       "       2, 2, 4, 2, 4, 4, 4, 2, 2, 3, 1, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2,\n",
       "       2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4,\n",
       "       2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 0, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4,\n",
       "       2, 2, 4, 2, 2, 4, 4, 4, 2, 0, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4,\n",
       "       2, 0, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2,\n",
       "       2, 3, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 2, 2, 4, 2, 4, 4, 2,\n",
       "       4, 1, 4, 2, 4, 2, 2, 2, 2, 2, 4, 4, 2, 4, 2, 0, 4, 4, 4, 2, 4, 3,\n",
       "       4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4,\n",
       "       2, 4, 4, 4, 4, 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting values into Labels and making the csv file for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl2=['elefante', 'farfalla','mucca','pecora','scoiattolo']\n",
    "predicted_values=[cl2[i] for i in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(predicted_values) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\n",
    "res.columns = [\"prediction\"]\n",
    "res.to_csv(\"prediction_Assignment2_102.csv\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
